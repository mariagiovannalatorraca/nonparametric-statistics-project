---
title: "Ground Motion Models for Italic Seismic data - Preliminary Step"
author: "Nonparametric Project"
date: "AA 2023-2024"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
source("functions/useful_function.R")
# devtools::install_github("ogru/fdagstat")
load_libraries()
```

Load data and inspect them

```{r}
seismic_data = readRDS("data/seismic_data.rds")
setDT(seismic_data)
attach(seismic_data)

seismic_data = data_preparation(seismic_data)
head(seismic_data)

```

Let us focus on the spectral acceleration (our target):

```{r}
SA = seismic_data[, 11:47]
periods = c(
  0, 0.01, 0.025, 0.04, 0.05, 0.07, 0.1, 0.15, 0.2, 0.25, 0.3,
  0.35, 0.4, 0.45, 0.5, 0.6, 0.7, 0.75, 0.8, 0.9, 1, 1.2, 1.4,
  1.6, 1.8, 2, 2.5, 3, 3.5, 4, 4.5, 5, 6, 7, 8, 9, 10
)

SA_log = log10(seismic_data[, 12:47])
logperiods <- log10(periods[2:37])

# Functional object definition
SA_f_data <- fData(logperiods, SA_log)    
plot(
  SA_f_data, xlab = "log_10(T)", ylab = "log_10(SA) - [cm/s2]", 
  main="Plot of Spectral Acceleration", col = rev(hcl.colors(5522, palette="Oranges"))
)

# Attempting the analysis with individual earthquakes
df_single_events <- aggregate(. ~ event_id, data = seismic_data[, c(1, 7:47)], mean)
df_single_events$sof <- as.numeric(df_single_events$sof)

```

# Data visualization

```{r}
IT_obs <- subset(seismic_data, ev_nation_code == "IT")
world_obs <- subset(seismic_data, ev_nation_code != "IT")

world_map <- map_data("world")
italy_map <- as.data.frame(subset(world_map, world_map$region=="Italy"))

```

## Plot of the Moment Magnitude on the Italian Map

```{r}
ggplot() +
  geom_polygon(data = italy_map, aes(x=long, y=lat, group=group), color = 'black', fill = 'white') + 
  geom_point(data = IT_obs, aes(x = event_lon, y = event_lat, color = magnitude), size = 4) +
  scale_color_gradient(low = "#FFC700", high = "#9D260C") +
  coord_equal() +
  labs(x = "Longitude", y = "Latitude", color = "magnitude") + theme_bw()

```

## Plot of type of faulting with respect to the Moment Magnitude on the Italian Map

```{r}
ggplot() +
  geom_polygon(data = italy_map, aes(x=long, y=lat, group=group), color = 'black', fill = 'white') + 
  geom_point(data = IT_obs, aes(x = event_lon, y = event_lat, size = magnitude, color = sof)) +
  scale_size(range = c(1, 5)) +
  scale_color_discrete() +
  coord_equal() +
  labs(x = "Longitude", y = "Latitude", color = "Type of faulting", size = "Moment Magnitude")
```

## Plot of the stations on the Italian Map

```{r}
ggplot() +
  geom_polygon(data = italy_map, aes(x=long, y=lat, group=group), color = 'black', fill = 'white') + 
  geom_point(data = IT_obs, aes(x = station_lon, y = station_lat)) +
  scale_color_gradient(low = "#FFC700", high = "#9D260C") +
  coord_equal() +
  labs(x = "Longitude", y = "Latitude", color = "vs30") + theme_bw()
```

## Boxplot of Moment Magnitude by style of faulting in the world

```{r}
ggplot(data = seismic_data, mapping = aes(x = sof, y = SA_1, fill = sof)) +
  geom_boxplot() + 
  scale_color_discrete() +
  xlab("Style of faulting") + 
  ylab("Moment Magnitude") + 
  ggtitle("Boxplot of Moment Magnitude by style of faulting in the world")

```

# Functional depth measures and exploration

Let us start by computing the MBD sample median of the SA functional data:

```{r}
median_curve <- median_fData(fData = SA_f_data, type = "MBD")
plot(
  SA_f_data, xlab = "log_10(T)", ylab = "log_10(SA) - [cm/s2]", lwd=0.05, 
  main="Plot of Spectral Acceleration", col = rev(hcl.colors(5522, palette="Oranges"))
)
lines(logperiods, median_curve$values, col='darkred', lwd = 3)
legend('bottomleft', col=c('darkred'), legend=c('Median'), lwd=c(3))
```

Based on this we can looking for outliers and eventually remove them from the dataset

```{r}
out_shape <- outliergram(SA_f_data, display = FALSE)
length(out_shape)

```

It is evident that there are outliers in the phase because an earthquake is perceived belatedly by the more distant stations. Therefore, it makes sense to retain all of them, also because removing some may result in the emergence of others, leading to the erroneous identification of two earthquakes.

# Permutational tests

## Analysis on magnitude

A hypothesis test is being conducted to assess whether the magnitude is less than 5.7 and, if so, whether there exists a difference in Seismic Activation (SA) compared to situations where the magnitude exceeds 5.7. The hypotheses can be formulated as follows:

-   Null Hypothesis ($H_0$): The magnitude is less than or equal to 5.7, and there is no discernible difference in Seismic Activation (SA).

-   Alternative Hypothesis ($H_1$): The magnitude is less than 5.7, and a significant difference in Seismic Activation (SA) exists.

The analysis utilizes the magnitude variable, with the Seismic Frequency (SF) variable reserved for subsequent analysis of variance (ANOVA).

```{r}
# Threshold value for magnitude
mh0 = 5.7

# Creating two sub-populations based on the magnitude threshold
SA_subpop1 <- seismic_data[which(magnitude < mh0), 12:47]
SA_subpop2 <- seismic_data[which(magnitude >= mh0), 12:47]

B <- 100
seed = 111222

# Performing the scalar multivariate testing
results.fd = ITP2bspline(
  SA_subpop1, SA_subpop2, mu = 0, order = 4, nknots = dim(SA_subpop1)[2], B = 5000, paired = FALSE
)
```
Plotting the p-values heatmap:
```{r}
ITPimage(results.fd, abscissa.range=range(logperiods))

# Selecting the significant components at 5% level
which(results.fd$corrected.pval < 0.05)
```

All calculated p-values are equal to 0, indicating a statistically significant difference between the two populations at every time point.

Attempting to Remove Spatial Correlation using a single event at a time:
```{r}
SA_single1 <- log10(df_single_events[which(df_single_events$magnitude < mh0), 7:42])
SA_single2 <- log10(df_single_events[which(df_single_events$magnitude >= mh0), 7:42])

ITP.result_single <- ITP2bspline(SA_single1, SA_single2, mu = 0, order = 4, nknots = dim(SA_single1)[2] - 2, B = 5000, paired = FALSE)

which(ITP.result_single$corrected.pval < 0.05)
```

The observed p-values are nearly uniformly equal to 0, indicating significant differences between the two populations at each time point, except for time points 2, 3, 4, 5, and 35.

Graphically we get
```{r}
matplot(
  logperiods, t(rbind(SA_single1, SA_single2)), type='l', col=c('#FFC700','#9D260C'),
  lty=1, xlab='Log(T)', ylab='Magnitude'
)
legend(
  'bottomleft', c('Magnitude < 5.7', 'Magnitude > 5.7'), col=c('#FFC700','#9D260C'), lty=1, lwd=2
)

```

More in detail for the mean:
```{r}
t1.mean = colMeans(SA_single1)
t2.mean = colMeans(SA_single2)
Mh <- rep(mh0, times = 36)

matplot(
  logperiods, t(rbind(t1.mean - Mh, t2.mean - Mh)), type='l', col=c('#FFC700','#9D260C'),
  lty=1,xlab='Log(T)',ylab='Magnitude'
)
legend(
  'bottomleft', c('Magnitude < 5.7', 'Magnitude > 5.7'), col=c('#FFC700','#9D260C'), lty=1, lwd=2
)
```

And also for the median:
```{r}
sa1_median <- median_fData(fData = fData(logperiods,SA_single1), type = "MBD")
sa2_median <- median_fData(fData = fData(logperiods,SA_single2), type = "MBD")

matplot(
  logperiods, t(rbind(sa1_median$values,sa2_median$values)), type='l', col=c('#FFC700','#9D260C'),
  lty=1, xlab='Log(T)', ylab='Magnitude'
)
legend(
  'bottomleft', c('Magnitude < 5.7', 'Magnitude > 5.7'), col=c('#FFC700','#9D260C'), lty=1, lwd=2
)
```

Now, the results are consistent, indicating that for events with magnitude less than 5.7 and low SA, there is no significant difference between the mean and median. However, it's worth noting that despite the similarity between mean and median values, the populations themselves exhibit significant differences.


## Manova on Style of Faulting
```{r}
B <- 5000
seed = 111222

nknots <- dim(model.response(model.frame(as.matrix(SA_log) ~ seismic_data$sof)))[2] - 2

ITP.aov <- ITPaovbspline(
  as.matrix(SA_log) ~ seismic_data$sof, order = 4,
  nknots = nknots, B = B, method = "residuals"
)
summary(ITP.aov)
```

Identify factors with corrected p-values less than 0.05:
```{r}
which(ITP.aov$corrected.pval.factors < 0.05)
```
The functions appear to diverge mainly towards the end of the observation period (32 and 36).

Attempting the analysis with individual earthquakes.
```{r}
B = 5000

SA_log_single = log10(df_single_events[, 7:42])

nknots = dim(model.response(model.frame(as.matrix(SA_log_single) ~ df_single_events$sof)))[2] - 2

ITP.aov_single <- ITPaovbspline(
  as.matrix(SA_log_single) ~ df_single_events$sof, order = 4,
  nknots = nknots, B = B, method = "residuals")
summary(ITP.aov_single)
```
Identify factors with corrected p-values less than 0.05:
```{r}
which(ITP.aov_single$corrected.pval.factors < 0.05)
```
All p-values are far from 0 (> 0.54), indicating that the SOF variable does not appear to be statistically significant.

Graphically we have:
```{r}
SA_subpop1 <- log10(df_single_events[which(df_single_events$sof==1), 7:42])
SA_subpop2 <- log10(df_single_events[which(df_single_events$sof==2), 7:42])
SA_subpop3 <- log10(df_single_events[which(df_single_events$sof==3), 7:42])

matplot(
  logperiods, t(rbind(SA_subpop1, SA_subpop2, SA_subpop3)), type='l', 
  col=c("#FFC700", "#FF9900", "#9D260C"), lty=1, xlab = 'Log(T)', ylab = 'log(SA)'
)
legend('bottomleft', c('TF', 'SS','NF'), col=c("#FFC700", "#FF9900", "#9D260C"), lty=1, lwd=2)
```
More in detail if we look at the mean:
```{r}
t1.mean = colMeans(SA_subpop1)
t2.mean = colMeans(SA_subpop2)
t3.mean = colMeans(SA_subpop3)

matplot(
  logperiods, t(rbind(t1.mean, t2.mean, t3.mean)), type='l',
  col=c("#FFC700", "#FF9900", "#9D260C"), lty=1, xlab = 'Log(T)',
  ylab = 'log(SA)'
)
legend('bottomleft', c('TF', 'SS','NF'), col=c("#FFC700", "#FF9900", "#9D260C"), lty=1, lwd=2)

```

and finally the medians:
```{r}
sa1_median <- median_fData(fData = fData(logperiods, SA_subpop1), type = "MBD")
sa2_median <- median_fData(fData = fData(logperiods, SA_subpop2), type = "MBD")
sa3_median <- median_fData(fData = fData(logperiods, SA_subpop3), type = "MBD")

matplot(
  logperiods, t(rbind(sa1_median$values, sa2_median$values, sa3_median$values)), type='l',
  col=c("#FFC700", "#FF9900", "#9D260C"), lty=1, xlab = 'Log(T)', ylab = 'log(SA)'
)
legend('bottomleft', c('TF', 'SS','NF'), col=c("#FFC700", "#FF9900", "#9D260C"), lty=1, lwd=2)

```
There is no difference between the mean and median plots, the curves overlap.


# Model
Split data in train-test in order to check how the model will perform on new data:
```{r}
test_index <- twin(seismic_data, r=5, u1=11)
train_set <- seismic_data[-test_index, ]
test_set <- seismic_data[test_index, ]
```

Select columns for fitting the model
```{r}
seismic_train_data <- data.frame(
  cbind(train_set[, c(7:10)], train_set[, 12:47])
)
```

## Model with SoF
First of all we define the design matrix
```{r}
Mh = 5.7
Mref = 4.5
model_design <- build_design_matrix(seismic_train_data, Mh, Mref, T)
design_matrix = model_design$design_matrix
```

### Multivariate Regression Model
Then on the above defined model we build the Bootstrap Confidence Intervals for the coefficients
```{r}
B <- 100
alpha <- 0.05
seed = 111222

names <- c("a", "f1", "f2", "c3", "c1", "c2", "k", "b1", "b2")

res = bootstrap_CI(design_matrix, names, alpha, B, seed, T)
```

### Functional Regression Model
Select the best smoothing basis by looking at one seismic event
```{r}
Xobs0 <- t(model_design$log10_SA[1,])

basis <- create.bspline.basis(logperiods, norder = 4)
opt.lambda <- get_optimal_lambda(-15, 0, 0.5, basis, Xobs0, logperiods)
```
```{r}
data.fd <- Data2fd(y = t(model_design$log10_SA), argvals = logperiods, basisobj = basis, lambda = opt.lambda)
basis  <- data.fd$basis
fPar <- fdPar(fdobj = basis, Lfdobj = 2, lambda = opt.lambda)
```

Plot the smoothing function and its derivative:
```{r}
Xss <- smooth.basis(logperiods, Xobs0, fPar)
Xss0 <- eval.fd(logperiods, Xss$fd, Lfd=0)
Xss1 <- eval.fd(logperiods, Xss$fd, Lfd=1)
Xss2 <- eval.fd(logperiods, Xss$fd, Lfd=2)

NT <- length(logperiods) 
rappincX1 <- (Xobs0[3:NT] - Xobs0[1:(NT-2)])/(logperiods[3:NT] - logperiods[1:(NT-2)])
rappincX2 <- (
  (Xobs0[3:NT] - Xobs0[2:(NT-1)])/(logperiods[3:NT] - logperiods[2:(NT-1)]) -
    (Xobs0[2:(NT-1)] - Xobs0[1:(NT-2)])/(logperiods[2:(NT-1)] - logperiods[1:(NT-2)])
  )*2/(logperiods[3:(NT)] - logperiods[1:(NT-2)])

par(mfrow=c(1,3))
plot(logperiods, Xobs0, xlab="log(T)", ylab = "log(SA)")
lines(logperiods, Xss0, col="#FF9900", lwd=2)
plot(logperiods[2:(NT-1)], rappincX1, xlab="log(T)", ylab ="1st differences")
lines(logperiods, Xss1, col="#FF9900", lwd=2)
plot(logperiods[2:(NT-1)], rappincX2, xlab="log(T)", ylab ="2nd differences")
lines(logperiods, Xss2, col="#FF9900", lwd=2)
```

Perform the penalized smoothing of the data based on the above selected smoothing basis
```{r}
xlist <- list(
  "intercept" = rep(1, model_design$n),
  "source1"   = model_design$source*model_design$dummy1,
  "source2"   = model_design$source*model_design$dummy2,
  "sof"       = model_design$sof,
  "distance"  = model_design$distance,
  "path1"     = model_design$path1,
  "path2"     = model_design$path2,
  "site"      = model_design$site
)

blist  <- list(fPar, fPar, fPar, fPar, fPar, fPar, fPar, fPar)

mod <- fRegress(y = data.fd, xfdlist = xlist, betalist = blist)

names = c('a', 'b1', 'b2', 'f2', 'c3', 'c1', 'c2', 'k')

plot_functional_coeff(mod, -2, 1, 100, names)

```
### Residual decorrelation
Conduct residual decorrelation with the assistance of Matlab for computational issues
```{r}
# Save the original residuals and the their covariance matrix 
vgm = vgm(10.1, "Exp", 0.1, 6)
value = residual_decorrelation(train_set, mod, -2, 1, 100, vgm, "data/residuals.csv", "data/sigma.csv")
```
```{r}
# Import the decorrelated residuals and plot them
decorrelated_res <- read_csv("data/decorrelated_residuals.csv", col_names = FALSE)

plot(value$t_grid, t(decorrelated_res)[, 30], type='l', ylim=range(c(t(decorrelated_res)[, 30], value$res[, 30])),
     main='decorrelated residuals wrt the originals', lwd=2, col='darkred', 
     xlab='log(T)', ylab='residual curve')
lines(value$t_grid, value$res[, 30], col='black', lwd=2)
legend("topright", col=c('black', 'darkred'), lwd=c(2, 2), legend=c('original', 'decorrelated'))
```
### Bootstrap Inference
```{r}
B <- 100
seed <- 111222

sqrt_root_sigma <- read_csv("data/squared_root_sigma.csv", col_names = FALSE)

test_res = functional_bootstrap_significance_test(
  logperiods,
  basis, mod, opt.lambda, 
  res, sqrt_root_sigma,
  as.matrix(design_matrix), 
  8, 4, value, 
  B, seed
)
```
Histogram of the test statistics distribution
```{r}
hist(test_res$t_stat, main='Histogram of the test statistics distribution', xlim=range(c(test_res$t_stat, test_res$t_obs)))
abline(v=test_res$t_obs, col=3)
```
```{r}
(pval = test_res$p_val)
```
Therefore we can remove sof as regressor

# Model without SoF
First of all we define the design matrix
```{r}
Mh = 5.7
Mref = 4.5
model_design <- build_design_matrix(seismic_train_data, Mh, Mref, F)
design_matrix = model_design$design_matrix
```

Perform the penalized smoothing of the data based on the above selected smoothing basis
```{r}
xlist.no_sof <- list(
  "intercept" = rep(1, model_design$n),
  "source1"   = model_design$source*model_design$dummy1,
  "source2"   = model_design$source*model_design$dummy2,
  "distance"  = model_design$distance,
  "path1"     = model_design$path1,
  "path2"     = model_design$path2,
  "site"      = model_design$site
)

blist.no_sof  <- list(fPar, fPar, fPar, fPar, fPar, fPar, fPar)

mod.no_sof <- fRegress(y = data.fd, xfdlist = xlist.no_sof, betalist = blist.no_sof)

names = c('a', 'b1', 'b2', 'c3', 'c1', 'c2', 'k')

plot_functional_coeff(mod.no_sof, -2, 1, 100, names)

```

## Residual decorrelation
Conduct residual decorrelation with the assistance of Matlab for computational issues
```{r}
# Save the original residuals and the their covariance matrix 
vgm = vgm(10.1, "Exp", 0.1, 9)
value.no_sof = residual_decorrelation(train_set, mod.no_sof, -2, 1, 100, vgm, "data/no_sof_residuals.csv", "data/no_sof_sigma.csv")
```

```{r}
# Import the decorrelated residuals and plot them
decorrelated_res <- read_csv("data/no_sof_decorrelated_residuals.csv", col_names = FALSE)

plot(
  value.no_sof$t_grid, t(decorrelated_res)[, 30], 
  type='l', ylim=range(c(t(decorrelated_res)[, 30], value.no_sof$res[, 30])),
  main='decorrelated residuals wrt the originals', lwd=2, col='darkred', 
  xlab='log(T)', ylab='residual curve'
)
lines(value.no_sof$t_grid, value.no_sof$res[, 30], col='black', lwd=2)
legend("topright", col=c('black', 'darkred'), lwd=c(2, 2), legend=c('original', 'decorrelated'))
```

## Functional Bootstrap Regression
```{r}
B = 100
seed = 111222

# Number of predictors
n_coeffs = length(names)

sqrt_root_sigma_no_sof <- read_csv("data/no_sof_squared_root_sigma.csv", col_names = FALSE)
t_grid = value.no_sof$t_grid

par(mfrow=c(2, 4), mar = c(3, 3, 2, 1))

for (j in c(1:n_coeffs)){
  functional_bootstrap_CI(
    t_grid, basis, mod.no_sof, 
    opt.lambda, xlist.no_sof, 
    blist.no_sof, j,  
    sqrt_root_sigma_no_sof, names, 
    B, seed
  )
}
```

